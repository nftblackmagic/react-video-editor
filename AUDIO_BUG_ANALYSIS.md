# Audio Playback Bug - Complete Code Analysis

## Problem Description
- Audio plays with sound initially after upload
- Sound disappears after page refresh
- Clicking in middle of timeline doesn't play audio from that position
- Console shows player mounting correctly but no audio
- Audio URLs are valid Bytescale URLs (not blob:// URLs that would expire)
- The Remotion Player mounts successfully with all methods available

## Environment & Technology Stack
- **Framework**: Next.js 15 with React 18
- **Video Engine**: Remotion 4.0
- **State Management**: Zustand + DesignCombo SDK
- **File Storage**: Bytescale CDN (persistent URLs)
- **Browser Environment**: Running in development mode (pnpm dev)
- **FPS**: 30 frames per second (constant throughout app)

## Related Code Files

### 1. Audio Component (`/src/features/editor/player/items/audio.tsx`)
```typescript
import { IAudio } from "@designcombo/types";
import { Audio as RemotionAudio } from "remotion";
import { BaseSequence, SequenceItemOptions } from "../base-sequence";
import React, { useEffect } from "react";

const AudioComponent = (props: {
	item: IAudio;
	options: SequenceItemOptions;
}) => {
	// Add safety checks
	if (!props) {
		console.error("âŒ Audio component called without props");
		return null;
	}

	const { item, options } = props;

	if (!item) {
		console.error("âŒ Audio component called without item");
		return null;
	}

	if (!options) {
		console.error(
			`âŒ Audio component called without options for item ${item.id}`,
		);
		return null;
	}

	const { fps } = options;
	const { details } = item;

	if (!details || !details.src) {
		console.error(`âŒ Audio item ${item.id} has no details or src`);
		return null;
	}

	// Debug logging for audio component - only on mount
	useEffect(() => {
		console.log("ðŸŽµ Audio component mounted:", {
			id: item.id,
			src: details.src,
			srcType: details.src.startsWith("blob:")
				? "BLOB URL (WILL EXPIRE!)"
				: "Regular URL",
			volume: details.volume ?? 100,
			calculatedVolume: (details.volume ?? 100) / 100,
			playbackRate: item.playbackRate || 1,
			trim: item.trim,
			display: item.display,
		});

		return () => {
			console.log("ðŸŽµ Audio component unmounted:", item.id);
		};
	}, [item.id]); // Only log when component mounts/unmounts

	const playbackRate = item.playbackRate || 1;

	// Check if URL is valid
	if (!details.src) {
		return null;
	}

	// Calculate proper frame values for trim (how much to skip from the original audio)
	// Note: startFrom is how many frames to skip from the beginning of the audio file
	// endAt is NOT used here - the duration is controlled by the Sequence wrapper
	const startFromFrame = item.trim?.from
		? Math.round((item.trim.from / 1000) * fps)
		: 0;

	// Log startFrom calculation for debugging
	useEffect(() => {
		console.log("ðŸŽ§ Audio playback parameters:", {
			id: item.id,
			startFromFrame,
			trimFrom: item.trim?.from,
			trimTo: item.trim?.to,
			displayFrom: item.display.from,
			displayTo: item.display.to,
			fps,
		});
	}, [startFromFrame, item.id]);

	const children = (
		<RemotionAudio
			startFrom={startFromFrame}
			playbackRate={playbackRate}
			src={details.src}
			volume={(details.volume ?? 100) / 100}
			muted={false}
		/>
	);
	return BaseSequence({ item, options, children });
};

// Memoize the component to prevent unnecessary re-renders
const Audio = React.memo(AudioComponent, (prevProps, nextProps) => {
	// Only re-render if item or options actually changed
	return (
		prevProps.item.id === nextProps.item.id &&
		prevProps.item.details.src === nextProps.item.details.src &&
		prevProps.item.details.volume === nextProps.item.details.volume &&
		prevProps.item.playbackRate === nextProps.item.playbackRate &&
		prevProps.item.display.from === nextProps.item.display.from &&
		prevProps.item.display.to === nextProps.item.display.to &&
		prevProps.item.trim?.from === nextProps.item.trim?.from &&
		prevProps.item.trim?.to === nextProps.item.trim?.to &&
		prevProps.options.fps === nextProps.options.fps
	);
});

Audio.displayName = "Audio";

export default Audio;
```

### 2. Base Sequence (`/src/features/editor/player/base-sequence.tsx`)
```typescript
import { ISize, ITrackItem } from "@designcombo/types";
import { AbsoluteFill, Sequence } from "remotion";
import { calculateFrames } from "../utils/frames";
import { calculateContainerStyles } from "./styles";

export interface SequenceItemOptions {
	handleTextChange?: (id: string, text: string) => void;
	fps: number;
	editableTextId?: string | null;
	currentTime?: number;
	zIndex?: number;
	onTextBlur?: (id: string, text: string) => void;
	size?: ISize;
	frame?: number;
	isTransition?: boolean;
}

export const BaseSequence = ({
	item,
	options,
	children,
}: {
	item: ITrackItem;
	options: SequenceItemOptions;
	children: React.ReactNode;
}) => {
	const { details } = item as ITrackItem;
	const { fps, isTransition } = options;
	const { from, durationInFrames } = calculateFrames(
		{
			from: item.display.from,
			to: item.display.to,
		},
		fps,
	);
	const crop = details.crop || {
		x: 0,
		y: 0,
		width: item.details.width,
		height: item.details.height,
	};

	// For audio items, we don't need AbsoluteFill wrapper
	if (item.type === "audio") {
		return (
			<Sequence
				key={`audio-${item.id}`}
				from={from}
				durationInFrames={durationInFrames || 1}
				// Ensure audio doesn't get recreated unnecessarily
				layout="none"
			>
				{children}
			</Sequence>
		);
	}

	return (
		<Sequence
			key={item.id}
			from={from}
			durationInFrames={durationInFrames || 1 / fps}
			style={{
				pointerEvents: "none",
			}}
		>
			<AbsoluteFill
				id={item.id}
				data-track-item="transition-element"
				className={`designcombo-scene-item id-${item.id} designcombo-scene-item-type-${item.type}`}
				style={calculateContainerStyles(details, crop, {
					pointerEvents: item.type === "audio" ? "none" : "auto",
				})}
			>
				{children}
			</AbsoluteFill>
		</Sequence>
	);
};
```

### 3. Sequence Item Registry (`/src/features/editor/player/sequence-item.tsx`)
```typescript
import React from "react";
import { IAudio, IImage, ITrackItem, IText, IVideo } from "@designcombo/types";
import { Audio, Image, Text, Video } from "./items";
import { SequenceItemOptions } from "./base-sequence";

export const SequenceItem: Record<
	string,
	(item: ITrackItem, options: SequenceItemOptions) => React.JSX.Element | null
> = {
	text: (item, options) => <Text item={item as IText} options={options} />,
	video: (item, options) => <Video item={item as IVideo} options={options} />,
	audio: (item, options) => <Audio item={item as IAudio} options={options} />,
	image: (item, options) => <Image item={item as IImage} options={options} />,
	// Subtitles are timeline-only, no preview rendering
	subtitle: (item, options) => null,
};
```

### 4. Composition (`/src/features/editor/player/composition.tsx`)
```typescript
import { SequenceItem } from "./sequence-item";
import React, { useEffect, useState, useMemo } from "react";
import { dispatch, filter, subject } from "@designcombo/events";
import {
	EDIT_OBJECT,
	EDIT_TEMPLATE_ITEM,
	ENTER_EDIT_MODE,
} from "@designcombo/state";
import { groupTrackItems } from "../utils/track-items";
import { calculateTextHeight } from "../utils/text";
import { useCurrentFrame } from "remotion";
import useStore from "../store/use-store";

const Composition = () => {
	const [editableTextId, setEditableTextId] = useState<string | null>(null);
	const {
		trackItemIds,
		trackItemsMap,
		fps,
		sceneMoveableRef,
		size,
		transitionsMap,
		structure,
		activeIds,
	} = useStore();
	const frame = useCurrentFrame();

	// Memoize grouped items to prevent recalculation on every frame
	const groupedItems = useMemo(
		() =>
			groupTrackItems({
				trackItemIds,
				transitionsMap,
				trackItemsMap: trackItemsMap,
			}),
		[trackItemIds, transitionsMap, trackItemsMap],
	);

	// Memoize media items
	const mediaItems = useMemo(
		() =>
			Object.values(trackItemsMap).filter((item) => {
				return item.type === "video" || item.type === "audio";
			}),
		[trackItemsMap],
	);

	const handleTextChange = (id: string, _: string) => {
		const elRef = document.querySelector(`.id-${id}`) as HTMLDivElement;
		const textDiv = elRef.firstElementChild?.firstElementChild
			?.firstElementChild as HTMLDivElement;

		const {
			fontFamily,
			fontSize,
			fontWeight,
			letterSpacing,
			lineHeight,
			textShadow,
			webkitTextStroke,
			textTransform,
		} = textDiv.style;
		const { width } = elRef.style;
		if (!elRef.innerText) return;
		const newHeight = calculateTextHeight({
			family: fontFamily,
			fontSize,
			fontWeight,
			letterSpacing,
			lineHeight,
			text: elRef.innerText || "",
			textShadow: textShadow,
			webkitTextStroke,
			width,
			id: id,
			textTransform,
		});
		elRef.style.height = `${newHeight}px`;
		sceneMoveableRef?.current?.moveable.updateRect();
		sceneMoveableRef?.current?.moveable.forceUpdate();
	};

	const onTextBlur = (id: string, _: string) => {
		const elRef = document.querySelector(`.id-${id}`) as HTMLDivElement;
		const textDiv = elRef.firstElementChild?.firstElementChild
			?.firstElementChild as HTMLDivElement;
		const {
			fontFamily,
			fontSize,
			fontWeight,
			letterSpacing,
			lineHeight,
			textShadow,
			webkitTextStroke,
			textTransform,
		} = textDiv.style;
		const { width } = elRef.style;
		if (!elRef.innerText) return;
		const newHeight = calculateTextHeight({
			family: fontFamily,
			fontSize,
			fontWeight,
			letterSpacing,
			lineHeight,
			text: elRef.innerText || "",
			textShadow: textShadow,
			webkitTextStroke,
			width,
			id: id,
			textTransform,
		});
		dispatch(EDIT_OBJECT, {
			payload: {
				[id]: {
					details: {
						height: newHeight,
					},
				},
			},
		});
	};

	//   handle track and track item events - updates
	useEffect(() => {
		const stateEvents = subject.pipe(
			filter(({ key }) => key.startsWith(ENTER_EDIT_MODE)),
		);

		const subscription = stateEvents.subscribe((obj) => {
			if (obj.key === ENTER_EDIT_MODE) {
				if (editableTextId) {
					// get element by  data-text-id={id}
					const element = document.querySelector(
						`[data-text-id="${editableTextId}"]`,
					);
					if (trackItemIds.includes(editableTextId)) {
						dispatch(EDIT_OBJECT, {
							payload: {
								[editableTextId]: {
									details: {
										text: element?.innerHTML || "",
									},
								},
							},
						});
					} else {
						dispatch(EDIT_TEMPLATE_ITEM, {
							payload: {
								[editableTextId]: {
									details: {
										text: element?.textContent || "",
									},
								},
							},
						});
					}
				}
				setEditableTextId(obj.value?.payload.id);
			}
		});
		return () => subscription.unsubscribe();
	}, [editableTextId]);

	return (
		<>
			{groupedItems.map((group, index) => {
				if (group.length === 1) {
					const item = trackItemsMap[group[0].id];
					if (!item) {
						return null;
					}

					// Check if handler exists for this item type
					if (!SequenceItem[item.type]) {
						return null;
					}

					const element = SequenceItem[item.type](item, {
						fps,
						handleTextChange,
						onTextBlur,
						editableTextId,
						frame,
						size,
						isTransition: false,
					});
					// Return null for subtitle items (no preview rendering)
					if (!element) return null;
					// Add key to the returned element
					return <React.Fragment key={item.id}>{element}</React.Fragment>;
				}
				return null;
			})}
		</>
	);
};

export default Composition;
```

### 5. Player Component (`/src/features/editor/player/player.tsx`)
```typescript
import { Player as RemotionPlayer, PlayerRef } from "@remotion/player";
import useStore from "../store/use-store";
import Composition from "./composition";
import { useEffect } from "react";

const Player = () => {
	const { duration, size, fps, playerRef, background } = useStore();

	// Add debugging to check player mount status
	useEffect(() => {
		console.log("ðŸŽ¬ Player component rendering");

		// Check if playerRef gets set
		const checkInterval = setInterval(() => {
			if (playerRef.current) {
				console.log("âœ… Remotion Player mounted successfully!", {
					playerRef: playerRef.current,
					hasPlayMethod: typeof playerRef.current.play === "function",
					hasPauseMethod: typeof playerRef.current.pause === "function",
					hasSeekToMethod: typeof playerRef.current.seekTo === "function",
				});
				clearInterval(checkInterval);
			}
		}, 100);

		// Cleanup after 5 seconds if not found
		const timeout = setTimeout(() => {
			clearInterval(checkInterval);
			if (!playerRef.current) {
				console.error("âŒ Remotion Player failed to mount after 5 seconds");
			}
		}, 5000);

		return () => {
			clearInterval(checkInterval);
			clearTimeout(timeout);
			console.log("ðŸŽ¬ Player component unmounting");
		};
	}, []);

	return (
		<RemotionPlayer
			ref={playerRef}
			component={Composition}
			durationInFrames={Math.round((duration / 1000) * fps) || 1}
			compositionWidth={size.width}
			compositionHeight={size.height}
			className={`h-full w-full bg-[${background.value}]`}
			fps={fps}
			overflowVisible
			numberOfSharedAudioTags={10}
		/>
	);
};
export default Player;
```

### 6. Timeline Events Hook (`/src/features/editor/hooks/use-timeline-events.ts`)
```typescript
import {
	PLAYER_PAUSE,
	PLAYER_PLAY,
	PLAYER_SEEK,
	PLAYER_SEEK_BY,
	PLAYER_TOGGLE,
} from "@designcombo/state";
import { filter, subject } from "@designcombo/events";
import { useEffect } from "react";
import { PlayerRef } from "@remotion/player";
import {
	TIMELINE_PREFIX,
	TIMELINE_SEEK,
	TIMELINE_SEEK_BY,
} from "@designcombo/timeline";
import { TRANSCRIPT_SEEK_TIME } from "../store/use-transcript-store";

export const useTimelineEvents = (
	fps: number,
	playerRef: React.MutableRefObject<PlayerRef | null>,
) => {
	const getSafeCurrentFrame = (ref: typeof playerRef) => {
		try {
			return ref.current?.getCurrentFrame() || 0;
		} catch {
			return 0;
		}
	};

	useEffect(() => {
		const subscription = subject
			.pipe(filter(({ key }) => key.startsWith(TIMELINE_PREFIX)))
			.subscribe((obj) => {
			if (obj.key === TIMELINE_SEEK) {
				const time = obj.value?.payload?.time;
				if (playerRef?.current && typeof time === "number") {
					playerRef.current.seekTo((time / 1000) * fps);
				}
			}
		});
		const stateEvents = subject
			.pipe(filter(({ key }) => key.startsWith("PLAYER")))
			.subscribe((obj) => {
			if (obj.key === PLAYER_SEEK) {
				const time = obj.value?.payload?.time;
				if (playerRef?.current && typeof time === "number") {
					playerRef.current.seekTo((time / 1000) * fps);
				}
			} else if (obj.key === PLAYER_PLAY) {
				const event = obj.value?.payload?.event;
				console.log("ðŸŽ® PLAYER_PLAY event received:", {
					hasPlayerRef: !!playerRef?.current,
					event: event,
					eventType: event?.type,
					isTrusted: event?.isTrusted,
				});

				if (playerRef?.current) {
					try {
						playerRef.current.play(event);
						console.log("âœ… play() called successfully with event");
					} catch (error) {
						console.error("âŒ Error calling play():", error);
					}
				} else {
					console.warn("âš ï¸ Cannot play - playerRef.current is null");
				}
			} else if (obj.key === PLAYER_PAUSE) {
				playerRef?.current?.pause();
			} else if (obj.key === PLAYER_TOGGLE) {
				const event = obj.value?.payload?.event;
				console.log("ðŸŽ® PLAYER_TOGGLE event received with event:", event);

				if (playerRef?.current?.isPlaying()) {
					playerRef.current.pause();
				} else {
					playerRef?.current?.play(event);
				}
			} else if (obj.key === PLAYER_SEEK_BY) {
				const frames = obj.value?.payload?.frames;
				if (playerRef?.current && typeof frames === "number") {
					const safeCurrentFrame = getSafeCurrentFrame(playerRef);
					playerRef.current.seekTo(Math.round(safeCurrentFrame) + frames);
				}
			}
		});
		const timelineSeekByEvents = subject
			.pipe(filter(({ key }) => key === TIMELINE_SEEK_BY))
			.subscribe((obj) => {
			const frames = obj.value?.payload?.frames;
			if (playerRef?.current && typeof frames === "number") {
				const safeCurrentFrame = getSafeCurrentFrame(playerRef);
				playerRef.current.seekTo(Math.round(safeCurrentFrame) + frames);
			}
		});

		// Listen for seek/pause events from timeline
		const timelineSeekEvents = subject
			.pipe(filter(({ key }) => key === TIMELINE_SEEK))
			.subscribe((obj) => {
			const time = obj.value?.payload?.time;
			if (playerRef?.current && typeof time === "number") {
				// Seek to the specified time
				const frame = (time / 1000) * fps;
				playerRef.current.seekTo(frame);

				// Always pause after seeking from timeline
				playerRef.current.pause();
			}
		});

		// Listen for transcript seek events
		const transcriptSeekEvents = subject
			.pipe(filter(({ key }) => key === TRANSCRIPT_SEEK_TIME))
			.subscribe((obj) => {
				const time = obj.value?.payload?.time;
				if (playerRef?.current && typeof time === "number") {
					// Jump player to selected transcript time
					playerRef.current.seekTo((time / 1000) * fps);
					// The timeline will automatically follow the player position
					// through the existing playhead tracking mechanism
				}
			});

		return () => {
			subscription.unsubscribe();
			stateEvents.unsubscribe();
			timelineSeekByEvents.unsubscribe();
			timelineSeekEvents.unsubscribe();
			transcriptSeekEvents.unsubscribe();
		};
	}, [fps, playerRef]);
};
```

### 7. Timeline Header with Play Button (`/src/features/editor/timeline/header.tsx`)
```typescript
import { Button } from "@/components/ui/button";
import {
	Pause,
	Play,
	SkipBack,
	SkipForward,
	RotateCcw,
	Zap,
} from "lucide-react";
import { dispatch } from "@designcombo/events";
import {
	PLAYER_PAUSE,
	PLAYER_PLAY,
	PLAYER_SEEK_BY,
	PLAYER_TOGGLE,
	PLAYER_SEEK,
} from "@designcombo/state";
import useStore from "../store/use-store";
import React from "react";

const TimelineHeader = () => {
	const { playerRef, fps } = useStore();
	const isPlaying = playerRef?.current?.isPlaying() || false;

	const handlePlay = (event?: React.SyntheticEvent) => {
		console.log("ðŸŽ® Play button clicked", {
			event: event,
			eventType: event?.type,
			hasPlayerRef: !!playerRef?.current,
		});
		dispatch(PLAYER_PLAY, { payload: { event } });
	};

	const handlePause = () => {
		dispatch(PLAYER_PAUSE, { payload: {} });
	};

	const handleToggle = (event: React.SyntheticEvent) => {
		console.log("ðŸŽ® Toggle button clicked", {
			event: event,
			eventType: event?.type,
			isPlaying: playerRef?.current?.isPlaying(),
		});
		dispatch(PLAYER_TOGGLE, { payload: { event } });
	};

	const handleSkipBackward = () => {
		dispatch(PLAYER_SEEK_BY, { payload: { frames: -fps } });
	};

	const handleSkipForward = () => {
		dispatch(PLAYER_SEEK_BY, { payload: { frames: fps } });
	};

	const handleRestart = () => {
		dispatch(PLAYER_SEEK, { payload: { time: 0 } });
	};

	return (
		<div className="border-border flex h-12 items-center justify-between border-b px-4">
			<div className="flex items-center gap-1">
				<Button
					variant="ghost"
					size="icon"
					onClickCapture={handleRestart}
					className="h-8 w-8"
				>
					<RotateCcw className="h-4 w-4" />
				</Button>

				<Button
					variant="ghost"
					size="icon"
					onClickCapture={handleSkipBackward}
					className="h-8 w-8"
				>
					<SkipBack className="h-4 w-4" />
				</Button>

				<Button
					variant="ghost"
					size="icon"
					onClickCapture={handleToggle}
					className="h-8 w-8"
				>
					{isPlaying ? (
						<Pause className="h-4 w-4" />
					) : (
						<Play className="h-4 w-4" />
					)}
				</Button>

				<Button
					variant="ghost"
					size="icon"
					onClickCapture={handleSkipForward}
					className="h-8 w-8"
				>
					<SkipForward className="h-4 w-4" />
				</Button>

				<Button variant="ghost" size="icon" className="h-8 w-8">
					<Zap className="h-4 w-4" />
				</Button>
			</div>
		</div>
	);
};

export default TimelineHeader;
```

### 8. Timeline Component - Seeking Logic (`/src/features/editor/timeline/timeline.tsx`)
Key seeking code:
```typescript
const onRulerClick = (time: number) => {
    // Update player position directly
    playerRef?.current?.seekTo((time * fps) / 1000);

    // Dispatch TIMELINE_SEEK event for transcript sync
    dispatch(TIMELINE_SEEK, { payload: { time } });
};
```

### 9. Upload Handler (`/src/features/editor/menu-item/uploads.tsx`)
Audio handling code:
```typescript
const handleAddAudio = (audio: any) => {
    // Audio is already on timeline from upload, this is for manual re-add
    const srcAudio = audio.metadata?.uploadedUrl || audio.url;

    console.log("ðŸŽµ handleAddAudio called:", {
        audioId: audio.id,
        uploadId: audio.uploadId,
        url: audio.url,
        urlType: audio.url?.startsWith("blob:") ? "BLOB URL" : "Regular URL",
        uploadedUrl: audio.metadata?.uploadedUrl,
        uploadedUrlType: audio.metadata?.uploadedUrl?.startsWith("blob:")
            ? "BLOB URL"
            : "Regular URL",
        bytescaleUrl: audio.metadata?.bytescaleUrl,
        selectedSrc: srcAudio,
        selectedSrcType: srcAudio?.startsWith("blob:")
            ? "BLOB URL (WILL EXPIRE!)"
            : "Regular URL",
    });

    // Check if this audio has a transcription (use uploadId or id)
    const transcriptionKey = audio.uploadId || audio.id;
    const hasTranscription = transcriptions[transcriptionKey]?.length > 0;

    if (hasTranscription) {
        // If we have transcription, ask user if they want to add as segments
        const edus = transcriptions[transcriptionKey];
        // Extract flat words from EDUs for segment splitter
        const segments = edus.flatMap((edu) => edu.words || []);

        // For now, add segments when user clicks on audio with transcription
        import("../utils/segment-splitter").then(({ addSegmentedMedia }) => {
            addSegmentedMedia(audio, segments, {
                autoSplit: true,
                mergeShortSegments: true,
                minSegmentLength: 1000,
            });
        });
    } else {
        // No transcription, add as single item again
        dispatch(ADD_AUDIO, {
            payload: {
                id: generateId(),
                type: "audio",
                details: {
                    src: srcAudio,
                },
                metadata: {},
            },
            options: {},
        });
    }
};
```

## Key Issues Identified

### 1. StartFrom Calculation Problem
The audio component calculates `startFrom` based only on `item.trim?.from`:
```typescript
const startFromFrame = item.trim?.from
    ? Math.round((item.trim.from / 1000) * fps)
    : 0;
```
This doesn't account for the current playback position when seeking.

### 2. Missing Current Frame Context
The audio component doesn't use `useCurrentFrame()` from Remotion, so it can't sync with the player's current position.

### 3. No Web Audio API Configuration
The RemotionAudio component doesn't specify `useWebAudioApi` prop, which might cause issues with audio context initialization.

### 4. Seeking Logic
When clicking on timeline:
1. `onRulerClick` calls `playerRef.current.seekTo()`
2. This should update the player position
3. But audio component's `startFrom` is static and doesn't update

### 5. Volume Bug (Already Fixed)
Video component had: `volume={details.volume || 0 / 100}` which always evaluated to 0.
Fixed to: `volume={(details.volume ?? 100) / 100}`

## Console Logs Showing the Issue
```
ðŸŽ¬ Player component rendering
âœ… Remotion Player mounted successfully! {playerRef: PlayerRef, hasPlayMethod: true, hasPauseMethod: true, hasSeekToMethod: true}
ðŸŽµ Audio component mounted: {id: "...", src: "https://upcdn.io/...", srcType: "Regular URL", volume: 100, ...}
ðŸŽ§ Audio playback parameters: {id: "...", startFromFrame: 0, trimFrom: undefined, ...}
ðŸŽ® Play button clicked {event: SyntheticBaseEvent, eventType: "click", hasPlayerRef: true}
âœ… play() called successfully with event
[No audio plays when seeking to middle of timeline]
```

## Additional Context and Related Code

### 10. Zustand Store Definition (`/src/features/editor/store/use-store.ts`)
```typescript
interface ITimelineStore {
	duration: number;
	fps: number;
	scale: ITimelineScaleState;
	scroll: ITimelineScrollState;
	size: ISize;
	tracks: ITrack[];
	trackItemIds: string[];
	transitionIds: string[];
	transitionsMap: Record<string, ITransition>;
	trackItemsMap: Record<string, ITrackItem>;
	structure: ItemStructure[];
	activeIds: string[];
	timeline: Timeline | null;
	setTimeline: (timeline: Timeline) => void;
	setScale: (scale: ITimelineScaleState) => void;
	setScroll: (scroll: ITimelineScrollState) => void;
	playerRef: React.RefObject<PlayerRef> | null;
	setPlayerRef: (playerRef: React.RefObject<PlayerRef> | null) => void;
	sceneMoveableRef: React.RefObject<Moveable> | null;
	setSceneMoveableRef: (ref: React.RefObject<Moveable>) => void;
	setState: (state: any) => Promise<void>;
	compositions: Partial<IComposition>[];
	setCompositions: (compositions: Partial<IComposition>[]) => void;
	background: {
		type: "color" | "image";
		value: string;
	};
	viewTimeline: boolean;
	setViewTimeline: (viewTimeline: boolean) => void;
}

const useStore = create<ITimelineStore>((set) => ({
	// ... initial state
	fps: 30,
	playerRef: null,
	// ... other properties
}));
```

### 11. Frame Utilities (`/src/features/editor/utils/frames.ts`)
```typescript
export const calculateFrames = (
	display: { from: number; to: number },
	fps: number,
) => {
	const from = (display.from / 1000) * fps;
	const durationInFrames = (display.to / 1000) * fps - from;
	return { from, durationInFrames };
};
```

### 12. Time Utilities (`/src/features/editor/utils/time.ts`)
```typescript
export const getSafeCurrentFrame = (playerRef: any): number => {
	try {
		if (!playerRef?.current) {
			return 0;
		}

		const frame = playerRef.current.getCurrentFrame();

		// Check if frame is a valid finite number
		if (typeof frame !== "number" || !Number.isFinite(frame)) {
			console.warn("getCurrentFrame returned non-finite value:", frame);
			return 0;
		}

		// Ensure frame is non-negative
		return Math.max(0, frame);
	} catch (error) {
		console.error("Error getting current frame:", error);
		return 0;
	}
};
```

### 13. Current Frame Hook (`/src/features/editor/hooks/use-current-frame.tsx`)
```typescript
import { CallbackListener, PlayerRef } from "@remotion/player";
import { useCallback, useSyncExternalStore } from "react";
import { getSafeCurrentFrame } from "../utils/time";

export const useCurrentPlayerFrame = (
	ref: React.RefObject<PlayerRef> | null,
) => {
	const subscribe = useCallback(
		(onStoreChange: () => void) => {
			const { current } = ref || {};
			if (!current) {
				return () => undefined;
			}
			const updater: CallbackListener<"frameupdate"> = () => {
				onStoreChange();
			};
			current.addEventListener("frameupdate", updater);
			return () => {
				current.removeEventListener("frameupdate", updater);
			};
		},
		[ref],
	);
	const data = useSyncExternalStore<number>(
		subscribe,
		() => getSafeCurrentFrame(ref),
		() => 0,
	);
	return data;
};
```

### 14. Remotion Audio Props (from node_modules)
```typescript
export type RemotionMainAudioProps = {
    startFrom?: number;
    endAt?: number;
};

export type RemotionAudioProps = NativeAudioProps & {
    name?: string;
    volume?: VolumeProp;
    playbackRate?: number;
    acceptableTimeShiftInSeconds?: number;
    _remotionInternalNeedsDurationCalculation?: boolean;
    _remotionInternalNativeLoopPassed?: boolean;
    toneFrequency?: number;
    useWebAudioApi?: boolean;
    pauseWhenBuffering?: boolean;
    showInTimeline?: boolean;
    delayRenderTimeoutInMilliseconds?: number;
    delayRenderRetries?: number;
    loopVolumeCurveBehavior?: LoopVolumeCurveBehavior;
};
```

### 15. ADD_AUDIO Event Dispatches
Multiple locations dispatch ADD_AUDIO:
- `/src/features/editor/menu-item/audios.tsx`
- `/src/features/editor/menu-item/uploads.tsx`
- `/src/features/editor/store/use-upload-store.ts`
- `/src/features/editor/utils/segment-splitter.ts`
- `/src/features/editor/editor.tsx`
- `/src/features/editor/scene/droppable.tsx`

Example dispatch:
```typescript
dispatch(ADD_AUDIO, {
    payload: {
        id: generateId(),
        type: "audio",
        details: {
            src: srcAudio,
        },
        metadata: {},
    },
    options: {},
});
```

### 16. Upload Store - Audio Upload Handler
```typescript
// From use-upload-store.ts
// Add audio to timeline immediately after upload
const audioId = generateId();
dispatch(ADD_AUDIO, {
    payload: {
        id: audioId,
        type: "audio",
        details: {
            src: mediaUrl,
        },
        metadata: {},
    },
    options: {},
});
```

## Critical Observations

### The Core Problem
1. **Static startFrom Calculation**: The `startFrom` prop in the Audio component is calculated once based on `item.trim?.from` and never updates based on the current playback position.

2. **Missing Frame Context**: The audio component receives `frame` in options but doesn't use it. It should use this to determine the correct playback position.

3. **Sequence Wrapper Timing**: The BaseSequence component sets `from` and `durationInFrames` for when the audio should appear in the timeline, but this doesn't control the audio's internal playback position.

### Why Audio Works Initially But Not After Refresh
1. **Initial Upload**: When first uploaded, the player starts from frame 0, and the audio's `startFrom=0` matches.
2. **After Refresh**: When seeking to middle of timeline, the player jumps to that frame, but the audio component still has `startFrom=0` and tries to play from the beginning.
3. **The audio element is created within its Sequence timeframe**, but it always starts playing from its beginning rather than from the seeked position.

### The Remotion Architecture
- **Sequence Component**: Controls WHEN an element appears (timeline position)
- **startFrom Prop**: Controls WHERE in the media file to start playing from
- **Current Issue**: We're not adjusting `startFrom` based on seeking

## Data Flow Analysis

### Timeline Click â†’ Player Seek â†’ Audio Playback Flow

1. **User clicks on timeline** (`timeline.tsx:onClickRuler`)
   ```typescript
   const time = unitsToTimeMs(units, scale.zoom);
   playerRef?.current?.seekTo((time * fps) / 1000);
   dispatch(TIMELINE_SEEK, { payload: { time } });
   ```

2. **Timeline event handled** (`use-timeline-events.ts`)
   ```typescript
   if (obj.key === TIMELINE_SEEK) {
       const time = obj.value?.payload?.time;
       if (playerRef?.current && typeof time === "number") {
           playerRef.current.seekTo((time / 1000) * fps);
       }
   }
   ```

3. **Player seeks to frame** (Remotion internal)
   - Player updates its internal current frame
   - Player re-renders composition at new frame

4. **Audio component renders** (`audio.tsx`)
   ```typescript
   // PROBLEM: This is static!
   const startFromFrame = item.trim?.from
       ? Math.round((item.trim.from / 1000) * fps)
       : 0;

   <RemotionAudio
       startFrom={startFromFrame}  // Always same value!
       playbackRate={playbackRate}
       src={details.src}
       volume={(details.volume ?? 100) / 100}
       muted={false}
   />
   ```

### The Missing Link
The audio component needs to know:
1. **What frame the player is currently at** when it mounts
2. **How far into its own duration** it should start playing

Currently, it only knows its trim position, not the player's current position.

## Timeline Units and Time Conversion

```typescript
// From utils/timeline.ts
export function unitsToTimeMs(units: number, zoom = 1): number {
    const zoomedFrameWidth = PREVIEW_FRAME_WIDTH * zoom;
    const frames = units / zoomedFrameWidth;
    const ms = (frames / DEFAULT_FPS) * 1000;
    return ms;
}

export function timeMsToUnits(ms: number, zoom = 1): number {
    const seconds = ms / 1000;
    const frames = seconds * DEFAULT_FPS;
    const zoomedFrameWidth = PREVIEW_FRAME_WIDTH * zoom;
    return frames * zoomedFrameWidth;
}
```

## Data Type Definitions

### ITrackItemBase (from @designcombo/types)
```typescript
export interface ITrackItemBase {
    id: string;
    name: string;
    type: ItemType;
    preview?: string;
    display: IDisplay;         // { from: number; to: number; } - timeline position in ms
    duration?: number;
    trim?: ITrim;             // { from: number; to: number; } - trim within media in ms
    isMain?: boolean;
    animations?: {
        in: IBasicAnimation;
        out: IBasicAnimation;
        loop: IBasicAnimation;
    };
    playbackRate?: number;
    modifier?: IDisplay;
    details?: any;
    activeEdit?: boolean;
    metadata: Record<string, any>;
    transitionInfo?: {
        isFrom: boolean;
        isTo: boolean;
        transition: ITransition;
    };
}

export interface IAudio extends ITrackItemBase {
    type: "audio";
    details: IAudioDetails;
}

export interface IAudioDetails extends ICommonDetails {
    src: string;        // URL to audio file
    volume?: number;    // 0-100
}
```

## Possible Root Causes (Without Assumptions)

### 1. StartFrom Calculation Issue
- The `startFrom` prop is calculated as `Math.round((item.trim.from / 1000) * fps)`
- This only considers the trim position, not the current playback position
- When seeking, this value remains static

### 2. Remotion Audio Component Behavior
- Remotion's `<Audio>` component expects `startFrom` in frames
- The component may be designed to always start from the specified `startFrom` when mounted
- Re-mounting during seeking might reset playback to `startFrom`

### 3. Sequence Component Timing
- The `Sequence` wrapper controls WHEN audio appears (timeline visibility)
- It uses `from` and `durationInFrames` to determine visibility window
- The audio might be correctly positioned but not playing the right portion

### 4. Browser Audio Context Issues
- Web Audio API might not be properly initialized after page refresh
- Browser autoplay policies might block audio without user interaction
- The `numberOfSharedAudioTags={10}` configuration might affect audio element pooling

### 5. Frame Synchronization Problem
- The composition receives `frame` prop but audio component doesn't use it
- Missing synchronization between player's current frame and audio playback position

### 6. Component Re-rendering and Memoization
- React.memo prevents unnecessary re-renders but might also prevent necessary updates
- The memoization comparison might not include all relevant props

### 7. Event Flow and Timing
- Timeline click â†’ TIMELINE_SEEK event â†’ player.seekTo() â†’ composition re-render
- Audio component might mount before or after the seek completes

### 8. URL and Resource Loading
- URLs are confirmed to be Bytescale CDN URLs (not blob://)
- But audio element might not be properly loading or buffering

## Browser-Specific Audio Considerations

### Autoplay Policies
- **Chrome**: Requires user gesture for audio playback
- **Safari**: Strict autoplay restrictions, requires user interaction
- **Firefox**: More lenient but still has restrictions

### Web Audio API vs HTML Audio
- Remotion can use either Web Audio API or HTML5 Audio elements
- The `useWebAudioApi` prop controls this behavior
- Current code doesn't specify this prop, using default behavior

### Audio Context States
- **suspended**: Initial state, requires user gesture to resume
- **running**: Normal playback state
- **closed**: Context has been terminated

## Remotion Audio Architecture

### How Remotion Handles Audio
1. **Audio Pool**: Uses `numberOfSharedAudioTags` to manage audio elements
2. **Frame-based Playback**: All timing is frame-based, not time-based
3. **Sequence Visibility**: Components only render within their sequence window
4. **StartFrom Behavior**: Determines where in the media file to begin playback

### Key Remotion Props for Audio
```typescript
export type RemotionAudioProps = {
    src: string;                          // Audio file URL
    startFrom?: number;                   // Frame to start playback from
    endAt?: number;                       // Frame to end playback
    volume?: number | ((frame: number) => number);
    playbackRate?: number;
    muted?: boolean;
    acceptableTimeShiftInSeconds?: number;
    useWebAudioApi?: boolean;            // Force Web Audio API usage
    pauseWhenBuffering?: boolean;
    loop?: boolean;
}
```

## Debug Information Available

### Console Logs Show
```
ðŸŽ¬ Player component rendering
âœ… Remotion Player mounted successfully!
ðŸŽµ Audio component mounted: {src: "https://upcdn.io/...", volume: 100}
ðŸŽ§ Audio playback parameters: {startFromFrame: 0, display: {...}}
ðŸŽ® Play button clicked
âœ… play() called successfully with event
```

### What's Missing in Logs
- Actual HTML audio element state
- Audio context state (suspended/running)
- Whether audio elements are being created
- Network requests for audio files
- Audio buffering status

## Hypothesis Testing Needed

1. **Test if audio element exists in DOM**
   - Check `document.querySelectorAll('audio')`
   - Inspect audio element properties

2. **Test if seeking affects startFrom**
   - Log startFrom value after seeking
   - Check if component remounts on seek

3. **Test Web Audio API state**
   - Check `window.AudioContext` state
   - Try with `useWebAudioApi={false}`

4. **Test frame synchronization**
   - Use `useCurrentFrame()` in audio component
   - Calculate dynamic startFrom based on current frame

5. **Test without memoization**
   - Remove React.memo temporarily
   - Check if updates propagate correctly

## Solution Approaches (To Be Tested)

### Approach 1: Dynamic StartFrom
Calculate `startFrom` based on player's current position when audio enters its sequence window.

### Approach 2: Use Frame Context
Utilize Remotion's `useCurrentFrame()` to sync audio with playback position.

### Approach 3: Disable Web Audio API
Set `useWebAudioApi={false}` to use HTML5 audio elements directly.

### Approach 4: Manual Audio Control
Create custom audio controller that handles seeking independently.

### Approach 5: Force Re-mount
Force audio component to re-mount with correct startFrom after seeking.